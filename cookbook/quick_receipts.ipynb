{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use lmeval to directly generate a model answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: This is a cat. It has blue eyes.\n",
      "execution time: 2.03 - tokens: 24\n"
     ]
    }
   ],
   "source": [
    "from lmeval import Media, Modality, FileType\n",
    "from lmeval.models.gemini import GeminiModel\n",
    "\n",
    "# create a media from file to be passed to the model\n",
    "content = open('data/media/cat_blue.jpg', 'rb').read()\n",
    "media = Media(content=content,\n",
    "              modality=Modality.image,\n",
    "              filetype=FileType.jpeg)\n",
    "\n",
    "\n",
    "model = GeminiModel()\n",
    "answer = model.generate_text('What is this animal? what is the color of its eyes?',\n",
    "                             medias=media)\n",
    "print(f\"answer: {answer.answer}\")\n",
    "print(f\"execution time: {round(answer.steps[0].execution_time, 2)} - tokens: {answer.steps[0].total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a custom task\n",
    "Sometime it is useful to generate custom task to change how they are scorered or use a more complex prompt - e.g a red prompt. This is doable by using the base `Task()` class and specifying it's element manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<text_generation Task: Custom task>\n"
     ]
    }
   ],
   "source": [
    "from lmeval import Task, TaskType, ScorerType, get_scorer\n",
    "from lmeval.prompts.adversarial_prompts import AdversarialChainOfThoughtsPrompt\n",
    "\n",
    " # you can also use the base prompt class for deeper customization of the prompt\n",
    "template = \"{{question.question}} Let's think step by step and answer the question starting with Answer:\"\n",
    "prompt = AdversarialChainOfThoughtsPrompt(template=template)\n",
    "\n",
    "# define a custom scorer\n",
    "scorer = get_scorer(ScorerType.contain_text_insensitive)\n",
    "\n",
    "# configure the task\n",
    "task = Task(name='Custom task',\n",
    "            type=TaskType.text_generation,\n",
    "            prompt=prompt,\n",
    "            scorer=scorer)\n",
    "print(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark files manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.path: data/demo_benchmark.db\n",
      "  idx  name                  version    parent_dir      categories    questions    evaled models    model answers  path\n",
      "-----  --------------------  ---------  ------------  ------------  -----------  ---------------  ---------------  ----------------------\n",
      "    0  Cat Visual Questions             data                     1            2                2                4  data/demo_benchmark.db\n"
     ]
    }
   ],
   "source": [
    "from lmeval import list_benchmarks, list_benchmark_fileinfo\n",
    "bench_list = list_benchmarks('./data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get specific Benchmark files information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.path: data/demo_benchmark.db\n",
      "  id  name                                          size  modality    filetype    compressed?    encrypted\n",
      "----  ------------------------------------------  ------  ----------  ----------  -------------  -----------\n",
      "   1  key                                            418  data        json        True           False\n",
      "   2  media/c57d0f6882d12717ee40650cb781071f.jpg   86546  image       jpeg        True           True\n",
      "   3  media/434ed4339e6dd1da18fa66d5cac2edcc.jpg   79484  image       jpeg        True           True\n",
      "   4  metadata.json                                  228  data        json        True           False\n",
      "   5  stats.json                                     648  data        json        True           False\n",
      "   6  benchmark.json                                4283  data        json        True           True\n"
     ]
    }
   ],
   "source": [
    "list_benchmark_fileinfo(bench_list[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
